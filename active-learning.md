# awesome-active-learning
## 2022
##### NIPS, Efficient Active Learning with Abstention: noise-seeking in predicted labels.
##### CVPR, Alpha-mix
##### AAAI, BATL
##### NAACL, AcTUNE
##### ICLR, LOW-BUDGET Active LEarning via Wasserstein Distance: An integer programming approach (not finish reading)
##### ICLR, Information gain propagation: a new way to graph active learning with soft labels: maximizin information gain propagation using GNN and oracles' agreement IGP on pseudo labels
## 2021
##### ACM, A survey of Deep Active Learning
##### VLDB, LANCET: Labeling Complex Data at Scale: covariate-shift
##### **NIPS, Diversity Enhanced Active Learning with Strictly Proper Scoring Rules: negative mean proper scores and mean objective cost of uncertainty **
##### ICLR, Similarity Search for Efficient Active learing and Search of Rare concepts: good paper with theoretical explanation
## 2020
##### CVPR, Contextual Diversity for Active Learning: KL divergence with diverse contexts
##### AISTAT, Stopping criterion for active learning based on deterministic generalization bounds: Bayes-based
##### CVPR, Deep Active Learning for Biased Datasets via Fisher Kernel Self-Supervision: not finish reading
##### ICLR, Deep Batch Active Leanring by Diverse, Uncertain gradient lower bounds (BADGE):  cluster of gradient embeddings,  better than core-set
##### IJCAI, On Deep Unsupervised Active Learning: DUAL learn a nonlinear embedding, using distance-based loss function and clustering
## 2017
##### NIPS, Active Bias: Training more accurate neural networks by emphasizing high variance samples: SGD Sampled by Easiness and SGD Weighted by Easiness using a threshold intergrated with P(yi|xi)
## 2015
##### NIPS, Active Learning from Weak and Strong Labelers: learn from low error on data labeled to reduce the amount of label queries
## 2014
##### A method for stopping Active Learning Based on Stabilizing Predictions and the Need for User-Adjustable Stopping: using Kappa statistic

