## 2023
##### ICML, Mitigating Memorization of Noisy labels by clipping the model prediction
## 2022
##### ICML, Sequentail Covariate Shift Dection Using Classifier Two-Sample Tests
##### ICLR, PICO: Contrastive Label Disambiguation for partial label learning
##### CVPR, PNP: Robust Learning from Noisy Labels by Probabilistic Noise Prediction
##### IEEE Transactions on Neural Networks and Learning Systems, Understand deep learning via decision boundary
##### AAAI, Neighborhood-Adaptive Structure Augmented Metric Learning, gauss distribution
##### ACL, UCTopic: Unsupervised Contrastive Learning for Phrase Representations and Topic Mining
##### WSDM, Towards Robust Graph Neural Networks for Noisy Graphs with Sparse Labels
##### CVPR, Large-scale pre-trainingi for Person re-identification with noisy labels, https://github.com/DengpanFu/LUPerson-NL
## 2021
##### A survey of label-noise Representation learning: Past, Present and Future
##### CVPR, Improving Unsupervised Image Clustering With Robust Learning
##### ICML, Confidence scores make instance-dependent Label-noise learning possible
##### CVPR, Jo-SRC: A contrastive approach for combating noisy labels
##### Vitaly Feldman, Does Learning require memorization? A short tale about long tail
##### ICML, Class2Simi: A noise reduction perspective on learning with noisy labels
## 2020
##### NIPS, What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation: estimate closely-related sampled influence and memorization through long-tailed distribution
##### NIPS, A topological Filter for Learning with Label Noise: detect clean data with high probability
##### SIGUA: Forgetting May make learning with noisy labels more robust: self-teaching + backward correction
##### Vitaly Feldman, When is Memorization of Irrelevant Training Data necessary for High-accuracy learning?
##### NIPS, Debiased Contrastive Learning
## 2019
##### ICLR, Multi-classs classification without multi-class labels
##### WSDM, Spring- Electrical models for link prediction
##### ICML, Unsupervised Label Noise Modeling and Loss correction: lipschitz and generalization bound
##### NIPS, Are anchor points really indispensable in label-noise learning? lipschitz
##### UAI, Comparing EM with GD in Mixture Models of two components
## 2018
##### NIPS, Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels
##### ICML, Dimensionality-Driven learning with noisy labels, (need to read again for constructing images)
## 2017
##### ICLR, Understanding deep learning requires rethinking generalization (Google brain)
##### ICML, A closer Look at Memorization in Deep Networks
## 2010 
##### Unsupervised Feature Selection for Multi-Cluster Data: good example of the time with Laplace and Clustering  using L1, eigen
##### Vitaly Feldman, Harvard, Optimal Hardness Results for Maximizing Agreements with Monomials
##### A theory of learning from different domains: H-divergence (not finish reading)
## 2001
##### Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data, https://www.cs.columbia.edu/~jebara/6772/papers/crf.pdf
